import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from causalinference import CausalModel

# Generate synthetic dataset
np.random.seed(0)
n_samples = 1000
n_features = 5
X = np.random.randn(n_samples, n_features)
treatment = np.random.randint(2, size=n_samples)
effect = 2 * treatment
y = 5 * X[:, 0] + 3 * X[:, 1] + effect + np.random.randn(n_samples)

# Split data into treatment and control groups
X_treatment = X[treatment == 1]
y_treatment = y[treatment == 1]
X_control = X[treatment == 0]
y_control = y[treatment == 0]

# Fit a logistic regression model to estimate propensity scores
propensity_model = LogisticRegression()
X_all = np.concatenate([X_treatment, X_control], axis=0)
y_all = np.concatenate([np.ones(len(X_treatment)), np.zeros(len(X_control))])
propensity_model.fit(X_all, y_all)
propensity_scores = propensity_model.predict_proba(X_all)[:, 1]

# Perform matching based on propensity scores
nn = NearestNeighbors(n_neighbors=1)
nn.fit(X_control)
distances, indices = nn.kneighbors(X_treatment)
matched_control_indices = indices.flatten()
matched_control = X_control[matched_control_indices]
matched_control_y = y_control[matched_control_indices]

# Estimate treatment effect using causal inference package
cm = CausalModel(Y=y_treatment, D=treatment[treatment == 1], X=X_treatment)
cm.est_propensity()
cm.est_via_matching(matches=matched_control_indices)
print(cm.estimates)
